\newcommand{\textretrieval}{textretrieval-001}

\newcommand{\sustain}{sustain-001}

\newcommand{\UIUC}{UIUC}

\section{Experiment Results}
As an analysis tool, the TL-HMM model provides with us the following two
patterns to characterize student behavior:
\begin{enumerate*}[label=(\arabic*)]
  \item the latent state representations, and
  \item the latent state transitions.
\end{enumerate*}
Thus to evaluate the proposed model, we conduct experiments to
qualitatively analyze both types of patterns discovered from empirical MOOC
log data.

Specifically, we look at the MOOC logs associated with two different
Coursera MOOCs offered by \UIUC{}: \textretrieval{} and \sustain{}.  The
\textretrieval{} MOOC represents a highly technical computer science course,
where the \sustain{} MOOC is more representative of a humanities course. We
picked these two MOOCs because of their vastly different content domains.
Table~\ref{table:datasets} summarizes the two datasets we extracted from
the MOOCs.

\begin{table}
  \begin{center}
    \begin{tabular}{rrrr}
      \textbf{MOOC} & \textbf{Students} & \textbf{Sequences} & \textbf{Avg.
      $|\mathbf{s}|$}\\\hline
      \textretrieval{} & 18,941 & 85,240 & 7.31\\
      \sustain{} & 85,240 & 231,881 & 15.4
    \end{tabular}
    \caption{Statistics about the sequences extracted from the two MOOCs.}
    \label{table:datasets}
  \end{center}
\end{table}

\subsection{Latent State Representations}
The TL-HMM is meant to be a tool for exploratory analysis of student
behavior. As such, the number of states should be empirically set based on
the goal of analysis. A higher number of states will lead to a
finer-grained modeling of student behavior. In our experiments, we explored
using between 2--6 states. First, we fit a 6-state TL-HMM to the
\textretrieval{} sequence dataset and show some of the latent state
representations we find. We used the following ten actions as our action
set $\mathbf{A}$:
\begin{enumerate*}[label=(\arabic*)]
  \item quiz start,
  \item quiz submit,
  \item wiki (course material),
  \item forum list (view the list of all forums),
  \item forum thread list (view the list of all threads in a specific
    forum),
  \item forum thread view (view the list of posts within a specific
    thread),
  \item forum search (a search query issued against the forum),
  \item forum post thread (a new thread was posted),
  \item forum post reply (a new post was created within an existing
    thread), and
  \item view lecture (defined as either streaming or downloading a lecture
    video).
\end{enumerate*}

To visualize these Markov models that represent our latent states, we plot
them as a directed graph where we set the size of a node to be proportional
to its personalized Pagerank score~\cite{Page:1999:PageRank, Jeh:2003:WWW}
where the personalization vector is the initial state distribution for the
Markov model. We let the thickness of a directed edge $(u, v)$ reflect the
probability of taking that edge given that a random walk is currently at
note $u$ (as indicated by the transition matrix)\footnote{We do not plot
the transition probabilities directly within the figure to ease
readability; we instead will mention relevant transition probabilities in
the text as we discuss the plots. The plots were created using
python-igraph: \url{http://igraph.org/python/}.}.

\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/text-6state/state0.png}
    \caption{An example ``quiz taking'' state.}
    \label{fig:practice-quiz-state}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/text-6state/state1.png}
    \caption{An example ``lecture viewing'' state.}
    \label{fig:lecture-viewing-state}
  \end{subfigure}
  \caption{Two example states found by the 6-state TL-HMM. (The naming of
  these states reflects our own interpretation.)}
  \label{fig:states}
\end{figure*}

\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/text-6state/state4.png}
    \caption{A state from \textretrieval{}.}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/sustain-6state/state4.png}
    \caption{A state from \sustain{}.}
  \end{subfigure}
  \caption{Two similar example states found by the 6-state TL-HMM.}
  \label{fig:cross-course}
\end{figure*}

Figure~\ref{fig:states} includes two such representations we learned. Under
our interpretation, the first corresponds to a ``quiz taking'' state (it
has higher ``quiz: start'' and ``quiz: submit'' state probabilities than
the other five states) whereas the second corresponds to a ``lecture
viewing'' state. Our unsupervised method can uncover states that do indeed
correspond to student behavior modes that we would expect to find a priori.

We also argue that it is important that the latent state representation be
a Markov model rather than just a discrete distribution over actions in
$\mathbf{A}$ (as would be the case for a traditional single-layer HMM). We
can observe why if we take a closer look at each of the two latent state
representations in Figure~\ref{fig:states} and look at their forum
component (bottom right). We can see that the relative probability of the
forum activities is roughly the same between these states, but the
\emph{transitions} are quite different. In
Figure~\ref{fig:practice-quiz-state} we have a relatively low probability
of walking from the ``forum thread view'' action back to the ``forum thread
list'' action (see the bottom rightmost arc; transition probability $p
\approx 0.17$), but in Figure~\ref{fig:lecture-viewing-state} we actually
observe a much stronger link in this direction (transition probability $p
\approx 0.63$). This difference highlights an important distinction between
these two latent states: in the first you are more likely to visit the
forum \emph{looking for a particular post}, where in the second you are
more likely to visit the forum \emph{to browse existing posts}. Thus,
capturing the action transition matrix within a latent state is important
for capturing detailed insights involving bigrams of actions.

We can also use our model for cross-course behavior analysis. In
Figure~\ref{fig:cross-course} we show two latent state representations
learned by a 6-state TL-HMM, one from \textretrieval{} and one from
\sustain{}. These two states were chosen as they are the most similar
across the two courses. However, if we look at the transitions we can see
some important differences. First, in the state from \textretrieval{}, we
can see that the probability of returning to the course wiki after viewing
a lecture (transition probability $p \approx 0.24$) is considerably lower
than that probability in the state from \sustain{} (transition probability
$p \approx 0.57$). We also notice that the self-loop for staying in a
lecture activity in \textretrieval{} (transition probability $p \approx
0.70$) is significantly higher probability than it is in the state from
\sustain{} (transition probability $p \approx 0.34$).  This gives us some
insight into the lecture viewing behavior of students in these two MOOCs
which can possibly be a reflection of the course's structure (which, as
demonstrated in \citet{Davis:2016:EDM}, can differ substantially across
different MOOCs).  In \textretrieval{}, students are likely to view lecture
videos in succession directly without first visiting the course wiki,
whereas in \sustain{} students are much more likely to first return the
course wiki before watching the next lecture video. This observation would
be lost if we did not consider the transitions between the actions within
the latent states.

\subsection{Varying the Number of Latent States}
Our TL-HMM model has a parameter $K$ that sets the number of latent states
to be learned. We believe that this can allow our model to flexibly
discover patterns of different granularity, and we can show this by varying
$K$ for a particular course and observing how the latent state
representations evolve.

\afterpage{%
\begin{landscape}
\begin{figure*}
  \centering
  \begin{tabular}{cccc}
    \textbf{State 0 (Wiki)} & \textbf{State 1 (Lecture)}
    & \textbf{State 2 (Thread View)} & \textbf{State 3 (Lecture/Wiki)}\\\hline
    \includegraphics[width=0.30\textwidth]{figures/text-2state/state0.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/text-2state/state1.png}\\
    \includegraphics[width=0.30\textwidth]{figures/text-3state/state0.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/text-3state/state1.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/text-3state/state2.png}\\
    \includegraphics[width=0.30\textwidth]{figures/text-4state/state0.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/text-4state/state1.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/text-4state/state2.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/text-4state/state3.png}
  \end{tabular}
  \caption{The evolution of states for increasing $K$ for the
  \protect\textretrieval{} MOOC.\ Each row corresponds to the next value of
  $K$, starting from $K=2$. State ``names'' indicate the highest
  probability action(s) within the state.} % I don't know why \protect is needed
  \label{fig:text-state-evolution}
\end{figure*}
\end{landscape}
}

\afterpage{%
\begin{landscape}
\begin{figure*}
  \centering
  \begin{tabular}{cccc}
    \textbf{State 0 (Wiki)}
    & \textbf{State 1 (Lecture/Wiki)}
    & \textbf{State 2 (Thread List)}
    & \textbf{State 3 (Thread View)}\\\hline
    \includegraphics[width=0.30\textwidth]{figures/sustain-2state/state0.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/sustain-2state/state1.png}\\
    \includegraphics[width=0.30\textwidth]{figures/sustain-3state/state0.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/sustain-3state/state2.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/sustain-3state/state1.png}\\
    \includegraphics[width=0.30\textwidth]{figures/sustain-4state/state0.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/sustain-4state/state3.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/sustain-4state/state1.png}
    &
    \includegraphics[width=0.30\textwidth]{figures/sustain-4state/state2.png}
  \end{tabular}
  \caption{The evolution of states for increasing $K$ for the
  \protect\sustain{} MOOC.\ Each row corresponds to the next value of $K$,
  starting from $K=2$. State ``names'' indicate the highest
  probability action(s) within a state.} % I don't know why \protect is needed
  \label{fig:sustain-state-evolution}
\end{figure*}
\end{landscape}
}

In Figure~\ref{fig:text-state-evolution} we see\footnote{The graphs are
quite small, but the states are in the same positions as they were in
previous figures.} the evolution of the latent state
representations found for the \textretrieval{} MOOC.  With $K=2$ we uncover
a video watching pattern (state 1) and a course material browsing pattern
(state 0). However, we can see that when $K=3$ we are able to uncover a new
pattern involving forum behavior in state 3 (notice the node sizes on the
bottom right). As we increase $K$ to four, we can see that state 1 splits
into state 1 and state 3. These states appear quite similar at a glance,
but there are still some key differences. First, we can see that state 1
now has a non-negligible forum component, whereas state 3 has hardly any
weight on the forum.

We observe similar behavior in Figure~\ref{fig:sustain-state-evolution}
as we increase $K$ when fitting our TL-HMM on the \sustain{} MOOC.\ Again,
in the transition between $K=2$ and $K=3$ we discover forum behavior
patterns as a latent state. However, in the transition between $K=3$ to
$K=4$ we instead see a refining of that discovered forum state, where
state 3 captures asymmetric transition probabilities between ``forum thread
view'' and ``forum thread list''. These states could be seen as redundant,
in which case a  setting of $K=3$ may be more appropriate for the
\sustain{} dataset.

\subsection{Transitions Between Latent States}

A unique property of our model is its ability to capture transitions
between the \emph{behavior patterns themselves} that are captured by the
latent states. In Figure~\ref{fig:trans-avg} we show the latent state
transition diagram for a 3-state TL-HMM fit on \textretrieval{}. We can
immediately observe two things: \begin{enumerate*}[label=(\arabic*)]
  \item each latent state has a very high ``staying'' probability, and
  \item the prevalence of each latent state matches our intuition.
\end{enumerate*}
In particular, we can see that the forum browsing state (state 2) has
relatively lower probability than the other states as we might expect. It
also makes sense that state 0 has rather high probability as this state
likely captures all sequences where a student logged in to the platform and
then did nothing else (likely checking for updates). If we look at state 1
and state 3, their relative probabilities match our intuition as
well. State 1 seems to capture a more engaged browsing session, where there
is non-negligible probability associated with different activities such as
quiz taking and forum browsing and, importantly, these activities have high
probability symmetric edges (so students are taking quizzes one after the
other, or viewing forum threads in succession). By contrast, state 3 seems
to capture a more passive student, with negligible probability mass
associated with forum activity (with low symmetry in the edges). The link
between ``quiz submit'' and ``quiz start'' (indicating quiz repetition) is
also significantly lower than state 1.

\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth,trim={0 3cm 0 2cm}]{figures/trans-comp/trans-avg.png}
    \caption{\label{fig:trans-avg}}
  \end{subfigure}%
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth,trim={0, 3cm 0 2cm}]{figures/trans-comp/trans-perfect.png}
    \caption{\label{fig:trans-perfect}}
  \end{subfigure}%
  \begin{subfigure}[t]{0.33\textwidth}
    \includegraphics[width=\textwidth,trim={0, 3cm 0 2cm}]{figures/trans-comp/trans-low.png}
    \caption{\label{fig:trans-low}}
  \end{subfigure}
  \caption{The latent state transition diagrams for a 4-state TL-HMM fit to
  \protect\textretrieval{} for all students (a) compared to only ``perfect''
  students (b) and only ``low'' students (c).}
  \label{fig:trans-comp}
\end{figure*}

Thus, we might expect to see students that perform well in the course
preferring states 1 and 2 over states 0 and 3. To verify this, we took the
model we learned on the full training data and retrofit it to training data
consisting only of sequences produced by students in \textretrieval{} that
had perfect marks. To prevent the latent state meanings from drifting, we
forced the model parameters associated with their Markov model
representations to be fixed, in effect only learning initial and transition
probabilities for the top layer of our TL-HMM.\ We show the updated latent
state transition diagram in Figure~\ref{fig:trans-perfect}. We can clearly
see that the probability of state 2 has increased dramatically, consistent
with previous observations of the positive correlation between forum activity and
grades~\cite{Huang:2014:LAS}, while the probability of states 0 and 3 has
decreased. State 1 had its probability increase, but only very slightly.

In Figure~\ref{fig:trans-low} we plot the latent state transition diagram
for a second group of ``low'' students. These students were selected so
that they attempted all required quizzes in the course, but such that their
average quiz score was $\leq 70\%$. Here, we see that \emph{state 1} has a
large increase in size, where we might have expected state 3 to grow
instead.  However, there is an alternative explanation for this phenomenon.
Since state 1 seems to indicate a highly engaged student, it is a perfectly
reasonable explanation for the ``low'' student group as they are going to
be working hard to try to fill in the gaps in their knowledge. By contrast,
the ``perfect'' student group likely has many members who can take the quiz
more passively and get perfect marks, perhaps because they already know
much of the material being presented, or are just naturally strong and do
not require much background review to perform well. This also explains why
state 1 did not increase in size for the ``perfect'' group like we were
anticipating. \citet{Kizilcec:2017:CandE} observe similar phenomena with
the courses they studied where they find that certificate earning is
negatively correlated with help seeking behavior. Our model lends itself
well to discovering this potentially counter-intuitive insight directly
from data.

\begin{table}
  \centering
  \begin{tabular}{r|rrrrr}
    \textbf{Group} & \textbf{State 0} & \textbf{State 1}^\dagger &
    \textbf{State 2}^\dagger & \textbf{State 3} & \textbf{State 2
    $\rightarrow$ 2}^\dagger\\\hline
    \textbf{Perfect} & 975.3  & 1001.5 & 999.0  & 1056.5 & 939.6\\
    \textbf{Low}     & 1024.9 & 816.4  & 1230.5 & 1161.2 & 1187.4\\
  \end{tabular}
  \caption{Average rank for ``perfect'' and ``low'' student groups in the
  ranked lists associated with the four latent states found by a TL-HMM.
  $\dagger$ indicates statistically significant different mean ranks at $p
  < 0.01$ according to an unpaired $t$-test.}
  \label{table:mean-rank}
\end{table}

To quantify this finding, we perform the following experiment. First, we
select all students from \textretrieval{} who completed all of the quizzes
$\mathbf{L}_q \subset \mathbf{L}$. This gives us 1,985 students along with
their average quiz score. We then create a ranked list of the students in
$\mathbf{L}_q$ by sorting them by their ``preference'' for a specific
latent state
\begin{equation}
  p_\ell(i) = \frac{\sum_{t=1}^T \gamma_t^{(\mathbf{o}_\ell)}(i)}
  {\sum_{j=1}^{K} \sum_{t=1}^T \gamma_t^{(\mathbf{o}_\ell)}(j)}
\end{equation}
where $\mathbf{o}_\ell$ is the list of action sequences for student $\ell$
and $\gamma$ is defined as before and computed using the Baum-Welch
algorithm. We can now compare the average rank in this list for both
the ``perfect'' student group and the ``low'' student group: a useful state
for distinguishing the two groups should result in a ranked list with
statistically significant differences in average rank between the two
groups. Our results are summarized in Table~\ref{table:mean-rank}. Indeed,
we discover that states 1 and 2 are correlated with the ``perfect'' or
``low'' groups: state 1 ranks students in the ``low'' group significantly higher
than those in the ``perfect'' group, and state 2 does the opposite and
prefers students in the ``perfect'' group to those in the ``low'' group.

Returning to Figure~\ref{fig:trans-comp}, we an also see a difference in
the transitions between the latent states. In particular, look at the
transitions in Figure~\ref{fig:trans-perfect} and
Figure~\ref{fig:trans-low} that leave state 2. In the ``perfect'' group,
nearly all of this probability mass is allocated for the self-loop ($p
\approx 0.93$). In the ``low'' group, this self loop is less strong ($p
\approx 0.80$; most easily seen by noting that the edges leaving state 2
are darker than for the ``perfect'' group). We can perform a similar
experiment to above by producing a ranked list of students $\ell \in
\mathbf{L}_q$ by their staying probability for state 2 (that is, given that
a student $\ell$ is already in state 2, how likely are they to remain there
in the next action sequence?). This can be computed as
\begin{equation}
  p_\ell(2, 2) = \frac{\sum_{t=1}^{T-1} \xi_t^{(\mathbf{o}_\ell)}(2,2)}
  {\sum_{i=1}^K \sum_{t=1}^{T-1} \xi_t^{(\mathbf{o}_\ell)}(2,i)}
\end{equation}
where $\xi$ is defined as before and computed using the Baum-Welch
algorithm. The last column of Table~\ref{table:mean-rank} indicates that
this transition feature also correlates with the achievement group and
results in significant differences in mean rank.
