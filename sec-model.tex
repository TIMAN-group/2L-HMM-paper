\section{A Two-Layer HMM for MOOC Log Analysis}

\subsection{Basic idea and rationale}

Our general idea is to use a probabilistic generative model to model the
student activities as recorded in a MOOC log, which means we will assume
that all the observed student activities are samples drawn (i.e.,
``generated'') from a parameterized probabilistic model. We can then
estimate the parameter values of the probabilistic model by fitting the
model to a specific MOOC log data set. The estimated parameter values could
then be treated as the latent ``knowledge'' discovered from the data.
Because such a generative model attempts to fit \emph{all} the data, it
enables us to discover interesting patterns that can explain the
\emph{overall} behavior of a student or the \emph{common} behavior patterns
shared by many students.

An HMM is a specific probabilistic generative model with a ``built-in''
state transition system that would control the data to be generated by the
model, thus it is especially suitable for modeling sequence
data~\cite{Rabiner:1990:RSR, Huang:1990:HMM}. At any moment, the HMM would
be in one of $k$ states $U=\{u_1,\ldots,u_k\}$, and at the next moment, the
HMM would move to another state stochastically according to a transition
matrix that specifies the probability $p(u_i \mid u_j)$ of going to state
$u_i$ when the HMM is currently in state $u_j$. When the HMM is in state
$u$, the HMM can generate an observable data point $x$ according to an
output probabilistic model $p(x \mid u)$. Thus if we ``run'' an HMM for $N$
time points denoted by $t=1,\ldots,N$, the HMM could ``generate'' a sequence
of observations $x_1 \ldots x_N$, where each $x_i$ is an output symbol by
going through a sequence of \emph{hidden states} $w_1 \ldots w_N$ where
$w_i \in U$ is a state. The association of such a latent sequence of state
transitions with the observed symbols makes it possible to use the HMM to
``decode'' the latent behavior of students behind the surface behavior we
directly observe in the log data, allowing for understanding student
behavior more deeply than a model with no latent state variables.

In many ways, the generation process behind an HMM is meant to simulate the
actual behavior of a student. We may say that students transition through
different ``task states'' (or ``behavior states'')  in the process of
study.  One such task state may be to learn about a topic by mostly
watching lecture videos, another task state may be to work on quizzes, and
yet another may be to participate in forum discussions. While in each of
these different states, the student would tend to exhibit different surface
``micro'' behaviors. For example, in the lecture study state, the student
would tend to have many video-watching related behaviors and occasionally
forum activities, while in the quiz-taking state (in order to pass each
module), the student would tend to show many quiz-related ``micro''
activities as well as asking questions or checking discussions on the
forum. Note that due to the complexity of the student behavior, it is very
difficult to accurately \emph{prescribe} the specific surface ``micro''
behavior patterns for each state in advance, especially without  prior
knowledge about the students. For example, forum activities are likely
interleaved with other activities   in every task state and the
interleaving pattern can be somewhat irregular with potentially many
variations.  The major motivations for using an HMM are that
\begin{enumerate*}[label=(\arabic*)]
  \item it uses a probabilistic model (the output probability distribution
    $p(x \mid u)$ conditioned on each state) to directly capture the
    inevitable uncertainty in the association of surface ``micro''
    activities with their corresponding latent task/behavior state, which
    is often our main target to discover and characterize, and
  \item it does not make any assumption about which latent task/behavior
    state must be associated with which observed activities or how a
    student would move from one state to another, but instead allows our
    data to ``tell'' us what kind of associations are most likely, what
    kind of transitions are most probable, and which states tend to be more
    long-lasting for any particular set of students.
\end{enumerate*}

However, if we use an ordinary HMM to analyze our data, we would treat each
observed ``micro'' activity (such as video watching or forum post reading)
as an output symbol, and thus the output distribution $p(x \mid u)$ for each
discovered latent state would be a simple distribution over all kinds of
observable micro activities recorded in our log data (e.g., 50\% lecture
watching, 8\% quiz taking, 7\% quiz submission, 2\% course wiki reading
\ldots). While such a distribution is meaningful and can already help us
interpret the corresponding latent state, it only gives us a rather
superficial characterization of student behavior.

Ideally, we want $p(x \mid u)$ to characterize the directly observable
``micro'' behavior in more detail to further capture the relations and
dependencies of these ``micro'' activities. To this end, we would treat an
\emph{entire sequence} of ``micro'' activities (e.g., one session of
activities) as an observed ``symbol'' from a latent state, and further
model the generation of such a sequence with another Markov model where we
treat each micro activity as an \emph{observable} state, and model the
transitions between these activity states in very much the same way as the
state transitions in an HMM.

Adding this second layer would allow us to characterize a latent task
state in much more detail, as it would reveal not only what activities are
most common to a task state, but also the transition patterns between these
``micro'' activities (e.g., it can reveal frequent back-and-forth
transitions between quiz-taking and quiz-submission, which would suggest a
concentrated period of taking quizzes). Combining this ``surface'' Markov
model over the ``micro'' actions with the ``deep'' hidden Markov model over
the latent task states gives us a fairly general and powerful two-layer HMM
(2L-HMM) that can simultaneously learn ``deeply'' the latent task/behavior
states and their transitions as well as the corresponding ``micro''
activity transition patterns associated with each latent state to
facilitate interpretation and analysis of the discovered latent state
patterns.

To estimate the parameters of our model, we will use the EM
algorithm~\citep{Dempster:1977:JRSS} which allows us to perform maximum
likelihood estimation of the model in the presence of latent (unobserved)
variables. This algorithm, intuitively, works as follows: first, the model
parameters we wish to estimate are initialized to some random (but valid)
starting point. Then, we ``guess'' what the latent variable values might
be given the current model parameters. We can then use this guess to
re-estimate the model parameters, which will improve their accuracy
slightly. We can then use these newly estimated parameters to again
``guess'' what the latent variable values might be, and so on. We repeat
this process until the parameter estimates no longer shift by much (or,
equivalently, the log likelihood of the data does not improve by much). The
computation for the ``guessing'' of latent variable values is somewhat
involved in the case of HMMs (see Section~\ref{sec:param-estimation} for
the full details), but the general intuition behind the iterative
hill-climbing remains valid.

Next, we present this model more formally and discuss how to
estimate its parameters to uncover these latent patterns in an unsupervised
manner.

\subsection{Formal Definition of the 2L-HMM}

Given a MOOC log, we can define a set $\mathbf{A}$ of actions that a
student can take at any given time. For example, an action $a \in
\mathbf{A}$ might be ``viewing lecture,'' ``taking quiz,'' or ``viewing
forum.'' For each student in the course $\ell \in \mathbf{L}$, we then
extract a list of action sequences $\mathbf{O}_\ell$ that he or she
produced as observed in the log, where each sequence $\mathbf{o} \in
\mathbf{O}_\ell$ is itself a list of actions $(a_1, a_2, \ldots, a_T)$ with
each $a_i \in \mathbf{A}$.  Each sequence can be divided flexibly; in this
paper, we chose to denote the end of a sequence as occurring when no further
actions occur within a 10-hour window of time (and thus our sequences
roughly correspond to one day's worth of activity). This decision reflects
our desire to uncover latent state transition behavior at the granularity
of day-to-day behavior. A different segmentation strategy could be used to
uncover hour-by-hour behavior or week-by-week behavior, and this depends
entirely on the desired time resolution one wishes to be exhibited in the
transitions between the latent states. Different segmentation strategies
will result in different underlying training data, and thus different
meanings behind the patterns that the 2L-HMM will extract. The flexibility
of using different segmentation strategies is intentional as it allows a
user to adjust the segmentation as needed so as to obtain patterns at
different granularity levels.

Each sequence $\mathbf{o} \in \mathbf{O}_\ell$ is associated with a latent
state $u \in \{1,\ldots,K\}$ (where $K$ is a fixed constant picked in
advance). The actions within the sequence $\mathbf{o} = (a_1, a_2, \ldots,
a_T)$ are then modeled as a first-order Markov process conditioned upon $u$
where each action is drawn from a distribution conditioned upon the
previous action (except for the first which is sampled from an initial
starting distribution). We can write the parameters for the first-order
Markov model associated with latent state $u$ as $\lambda^{(u)} =
(\pi^{(u)}, A^{(u)})$ where $\pi^{(u)}$ indicates the initial probability
vector of length $|\mathbf{A}|$ and $A^{(u)}$ is an $|\mathbf{A}| \times
|\mathbf{A}|$ matrix indicating the transition probabilities between each
pair of actions from $\mathbf{A}$.

Thus, the probability of a sequence $\mathbf{o}$ of length $T$ given a
latent state $u$ is
\begin{equation}
  P(\mathbf{o} \mid \lambda^{(u)}) = P(a_1 \mid \pi^{(u)}) \prod_{i=2}^T P(a_i \mid
  a_{i-1}, A^{(u)})
\end{equation}
where $P(a \mid \pi^{(u)}) = \pi^{(u)}_a$ is the probability of starting
with action $a$ and $P(a_i \mid a_{i-1}, A^{(u)}) = A^{(u)}_{a_{i-1}, a_i}$
is the transition probability of moving from action $a_{i-1}$ to $a_{i}$
given that the model is currently in latent state $u$.

We can compute the likelihood of a list of action sequences
$\mathbf{O}_\ell$ of length $N$ for a student $\ell$ by marginalizing over
all possible latent state sequences $(v_1, \ldots, v_N) \in U$ as
\begin{equation}
  P(\mathbf{O}_\ell \mid \Lambda)
  = \sum_{(v_1, \ldots, v_N) \in U} \left(P(v_1 \mid \Lambda)
  P(\mathbf{o}_1 \mid \lambda^{(v_1)}) \times \prod_{i=2}^N P(v_i \mid
  v_{i-1}, \Lambda) P(\mathbf{o}_i \mid \lambda^{(v_i)})\right)
\end{equation}
where $\Lambda$ is the set of all model parameters.  In our model, we let
$\Lambda = (\pi, A, B)$ where $\pi$ and
$A$ are the parameters of a first-order Markov model over the latent states
and $B = (\lambda^{(1)}, \ldots, \lambda^{(K)})$ where each $\lambda^{(i)}$
consists of the parameters for the first-order Markov model over action
sequences for latent state $i$. Thus $\pi$ (without superscripts) is an
initial probability vector of length $K$ and $A$ (without superscripts) is
a $K \times K$ transition probability matrix, analogous to the case with
the individual first-order Markov model parameters $\lambda^{(i)}$ for each
latent state. We have in total $K + K^2 + K(|\mathbf{A}| + |\mathbf{A}|^2)$
parameters to be estimated from our sequence data.

This can be seen as a modification of the traditional hidden Markov
model with categorical outputs~\cite{Rabiner:1990:RSR} where instead of
discrete observations (one for each latent state transition) we have
observations that take the form of entire sequences $\mathbf{o}_i = (a_1,
a_2, \ldots a_T)$ whose probabilities are computed using another
(non-hidden) Markov model conditioned upon the latent state $u_i$. 


\subsection{Parameter Estimation}
\label{sec:param-estimation}
To learn the parameters of our model, we may use maximum likelihood
estimation. Unfortunately, a closed-form solution does not exist, so we
must appeal to the EM algorithm~\cite{Dempster:1977:JRSS}. In particular,
we propose a minor modification of the Baum-Welch
algorithm~\cite{Rabiner:1990:RSR} which is an efficient EM algorithm for
learning the parameters of hidden Markov models in an unsupervised setting
where the Markovian assumption is exploited to significantly reduce the
computational complexity of the EM algorithm by avoiding explicit
enumeration of all the possible state transitions.  In the following
sections, we will provide a brief description of the original Baum-Welch
algorithm for unsupervised parameter estimation for categorical valued
hidden Markov models, and then describe our modification to allow for
parameter estimation for our 2L-HMM modification for sequence valued
observations.

\subsubsection{Baum-Welch for Traditional HMMs}
In the traditional HMM formulation with categorical outputs, we have
$\Lambda = (\pi, A, B)$ where $\pi$ is the initial probability distribution
over the latent states (of length $K$), $A$ is a $K\times K$ matrix
indicating the latent state transition probabilities, and $B = (b_1,
\ldots, b_k)$ is a length $K$ vector where each entry $b_i$ is a
probability distribution over the possible discrete output symbols from
$\mathbf{A}$.

The goal of the Baum-Welch algorithm (also called the forward-backward
algorithm) is to learn the values for the parameters $\Lambda$ from a
collection of observed sequences of values from $\mathbf{A}$. Concretely,
our training data $D = (\mathbf{o}^{(1)}, \ldots, \mathbf{o}^{(M)})$ is a
collection of $M$ sequences $\mathbf{o}^{(k)}$, each of which consists of $T_k$
symbols (observations) from $\mathbf{A}$. The Baum-Welch algorithm defines
two sets of variables $\alpha^{(\mathbf{o})}_t(i)$ called the forward
variables and $\beta^{(\mathbf{o})}_t(i)$ called the backward
variables~\cite{Rabiner:1990:RSR} for each sequence $\mathbf{o} \in
\mathbf{D}$.

$\alpha^{(\mathbf{o})}_t(i) = P(o_1, \ldots, o_t, v_t = i \mid \Lambda)$ is
the probability of generating the sequence of observations $(o_1, o_2,
\ldots, o_t)$ up to time $t$ and arriving in state $i$ at that time. They
are typically defined using the following recursion:
\begin{itemize}
  \item $\alpha^{(\mathbf{o})}_1(i) = \pi_i b_i(o_1)$, the probability of
    starting in state $i$ ($\pi_i$) times the probability of generating the
    first observation $o_1$ from state $i$.
  \item $\alpha^{(\mathbf{o})}_{t+1}(i) = b_i(o_{t+1})\sum_{j=1}^K
    \alpha^{(\mathbf{o})}_t(j) A_{ji}$, the probability of generating the
    observation $o_{t+1}$ from state $i$ times the probability that we
    arrive in state $i$ from any other previous state after generating all
    of the other observations.
\end{itemize}

Analogously, $\beta^{(\mathbf{o})}_t(i) = P(o_{t+1}, \ldots, o_{T} \mid v_t
= i, \Lambda)$ is the probability of generating the \emph{rest of the
sequence} given that we are in state $i$ at time $t$. They are also defined
using a recursion:
\begin{itemize}
  \item $\beta^{(\mathbf{o})}_T(i) = 1$
  \item $\beta^{(\mathbf{o})}_t(i) = \sum_{j=1}^K
    \beta^{(\mathbf{o})}_{t+1}(j) A_{ij} b_j(o_{t+1})$, the probability of
    transitioning to any state $j$ and generating the observation $o_{t+1}$
    times the probability of generating the rest of the sequence given that
    we transitioned to state $j$.
\end{itemize}

Given the $\alpha$s and the $\beta$s, we can compute
$\gamma^{(\mathbf{o})}_t(i)$, the posterior probability of being in a given
state $i$ at time $t$, and $\xi^{(\mathbf{o})}_t(i,j)$, the posterior
probability of going through a transition from state $i$ to state $j$ at
time $t$ as
\begin{equation}
  \gamma^{(\mathbf{o})}_t(i) =
  \frac{\alpha^{(\mathbf{o})}_t(i)\beta^{(\mathbf{o})}_t(i)}{\sum_{j=1}^K
  \alpha^{(\mathbf{o})}_t(j)\beta^{(\mathbf{o})}_t(j)}
\end{equation}
and
\begin{equation}
  \xi^{(\mathbf{o})}_t(i,j) = \frac{\alpha^{(\mathbf{o})}_t(i) A_{ij}
  b_j(o_{t+1}) \beta^{(\mathbf{o})}_{t+1}(j)} {\sum_{j=1}^K
  \alpha^{(\mathbf{o})}_t(j)\beta^{(\mathbf{o})}_t(j)}
\end{equation}
respectively~\cite{Rabiner:1990:RSR}. Computing these variables for each
sequence $\mathbf{o} \in D$ is the E-step of the EM algorithm.

Given $\gamma^{(\mathbf{o})}_t(i)$ and $\xi^{(\mathbf{o})}_t(i,j)$ for each
sequence $\mathbf{o} \in \mathbf{D}$, we can update our model parameters
$\Lambda$ as
\begin{align}
  \pi_i &= \frac{\sum_{\mathbf{o} \in \mathbf{D}}
  \gamma^{(\mathbf{o})}_1(i)}{|\mathbf{D}|},\\
  A_{ij} &= \frac{\sum_{\mathbf{o} \in \mathbf{D}} \sum_{t=1}^T
  \xi^{(\mathbf{o})}_t(i,j)} {\sum_{\mathbf{o} \in \mathbf{D}} \sum_{j=1}^N
  \sum_{t=1}^T \xi^{(\mathbf{o})}_t(i,j)},
  \text{ and }\\
  b_i(a) &= \frac{\sum_{\mathbf{o} \in \mathbf{D}} \sum_{t=1, o_t = a}^T
  \gamma^{(\mathbf{o})}_t(i)} {\sum_{\mathbf{o} \in \mathbf{D}}
  \sum_{t=1}^T \gamma^{(\mathbf{o})}_t(i)}
\end{align}
respectively~\cite{Rabiner:1990:RSR}. This is the M-step of the EM
algorithm.

\subsubsection{Baum-Welch for HMMs with Sequence Observations}
The major deviation of our model from the traditional categorical-valued
HMM is that our observations are themselves sequences of actions from
$\mathbf{A}$ rather than individual tokens. We again denote our parameters
as $\Lambda = (\pi, A, B)$ where $\pi$ is the initial probability
distribution over the latent states (of length $K$), $A$ is a $K \times K$
matrix indicating the latent state transition probabilities, but $B =
(\lambda^{(1)}, \ldots, \lambda^{(K)})$ is now a vector of length $K$ where
each element $\lambda^{(i)}$ consists of the parameters for a first-order
Markov model over the action space $\mathbf{A}$. Recall that each
$\lambda^{(i)} = (\pi^{(i)}, A^{(i)})$ where $\pi^{(i)}$ is an initial
action distribution over the $|\mathbf{A}|$ available actions, and
$A^{(i)}$ is the $|\mathbf{A}| \times |\mathbf{A}|$ transition matrix
between those actions.

The goal of the Baum-Welch algorithm is still to learn the parameters
$\Lambda$ for our model. What differs from the categorical-valued HMM case
is that our data now consists of a collection of \emph{lists} of sequences,
rather than just a collection of sequences. This means that our observation
values $\mathbf{o}_t$ are now \emph{themselves} sequences of values from
$\mathbf{A}$, rather than just being a single token from $\mathbf{A}$.
Formally, our training data $\mathbf{D} = (\mathbf{O}^{(1)}, \ldots,
\mathbf{O}^{(|\mathbf{L}|)})$, where each $\mathbf{O}^{(\ell)} = (\mathbf{o}_1,
\ldots, \mathbf{o}_{T_\ell})$ is itself a list of sequences. Each sequence
$\mathbf{o}_k \in \mathbf{O}^{(\ell)}$ consists of a list of actions $(a_1,
\ldots, a_{T_k})$, each of which is a member of $\mathbf{A}$.

In order to run the forward-backward algorithm for an element $\mathbf{O}
\in \mathbf{D}$ to compute the $\alpha$ and $\beta$ recursions like before,
we must define $b_i(\mathbf{o}_t)$ in this setting where $\mathbf{o}_t =
(a_1, \ldots, a_{T_t})$ is now a sequence instead of a single token. We
define it as follows:
\begin{equation}
  b_i(\mathbf{o}_t) = P(\mathbf{o}_t \mid \lambda^{(i)})
  = P(a_1 \mid \pi^{(i)}) \prod_{k=2}^{T_t} P(a_k \mid a_{k-1}, A^{(i)})
  = \pi^{(i)}_{a_1} \prod_{k=2}^{T_t} A^{(i)}_{a_{k-1},a_{k}}.
\end{equation}
Fortunately, the recursions for $\alpha^{(\mathbf{O})}_t(i)$ and
$\beta^{(\mathbf{O})}_t(i)$ remain the same, as do the definitions of
$\gamma^{(\mathbf{O})}_t(i)$ and $\xi^{(\mathbf{O})}_t(i,j)$, in the
E-step. We can simply substitute the new definition for the output
distribution $b_i(\mathbf{o}_t)$ in those equations.

The substantial change is in the updating equations in the M-step,
where we replace the update for $b_i(a)$ (which used to be a categorical
distribution) by a pair of updates for the Markov chain for state $i$: one
for $\pi^{(i)}_a$ one for $A^{(i)}_{ab}$. The two updates can be written as
\begin{align}
  \pi^{(i)}_{a}
  &= \frac{\sum_{\mathbf{O} \in \mathbf{D}} \sum_{\mathbf{o}_t \in
  \mathbf{O},\mathbf{o}_{t,1} = a} \gamma^{(\mathbf{O})}_t(i)}
  {\sum_{\mathbf{O} \in \mathbf{D}} \sum_{\mathbf{o}_t \in \mathbf{O}}
  \gamma^{(\mathbf{O})}_t(i)}, \text{ and }\\
  A^{(i)}_{ab}
  &= \frac{\sum_{\mathbf{O} \in \mathbf{D}} \sum_{\mathbf{o}_t \in \mathbf{O}}
  \sum_{m=2,\mathbf{o}_{t,m-1}=a \land \mathbf{o}_{t,m} = b}^{|\mathbf{o}_{t}|}
  \gamma^{(\mathbf{O})}_t(i)}
  {\sum_{\mathbf{O} \in \mathbf{D}} \sum_{\mathbf{o}_t \in \mathbf{O}}
  \sum_{m=2,\mathbf{o}_{t,m-1}=a}^{|\mathbf{o}_{t}|} \gamma^{(\mathbf{O})}_t(i)}.
\end{align}
These updates can be understood as follows. $\pi^{(i)}_a$ is the
probability that a sequence being generated by state $i$ begins with action
$a$. $\gamma^{(\mathbf{O})}_t(i)$ gives the probability of generating the
sequence $\mathbf{o}_t$ from state $i$, so we simply aggregate this for all
sequences $\mathbf{o}_t$ where the first action is $a$. We then normalize
this distribution to sum to 1 across all possible actions $a \in
\mathbf{A}$ to obtain our new estimate for $\pi^{(i)}_a$.  $A^{(i)}_{ab}$
is the probability that a sequence being generated by state $i$ currently
at action $a \in \mathbf{A}$ transitions to action $b \in \mathbf{A}$.
Thus, we compute the expected number of times we observe a transition from
$a$ to $b$ in a sequence generated by state $i$, and we normalize this
distribution to sum to 1 across all possible actions $b \in \mathbf{A}$.
Our modified EM algorithm for 2L-HMMs is provided as part of the \meta/
toolkit~\citemeta{}.
