\section{A Two-Layer HMM for MOOC Log Analysis}

\subsection{Basic idea and rationale} 

Our general idea is to use a probabilistic generative model to model 
the student activities as recorded in a MOOC log, which means we will 
assume that all the observed student activities are samples drawn (i.e., ``generated'') from 
a parameterized probabilistic model. We can then estimate the 
parameter values of the probabilistic model by fitting the model to a specific
MOOC log data set. The estimated parameter values could then be treated as 
the latent ``knowledge'' discovered from the data. Because such a generative model 
attempts to fit {\em all} the data, it enables us to discover interesting patterns
that can explain the {\em overall} behavior of a student or the {\em common} behavior patterns shared by many students. 

An HMM  is a specific probabilistic generative model with a ``built-in'' state transition system
that would control the data to be generated by the model, thus it is especially 
suitable for modeling sequence data~\cite{Rabiner:1990:RSR, Huang:1990:HMM}. At any moment, the HMM would be in 
one of $k$ states $U=\{u_1, ...,u_k\}$, and at the next moment, the HMM would move to 
another state stochastically according to a transition matrix that specifies the probability of
going to state $u_i$ when the HMM is currently in state $u_j$, i.e., $p(u_i|u_j)$. 
When the HMM is in state $u$, the HMM can generate an observable data point $x$ 
according to an output probabilistic model $p(x|u)$. Thus if we ``run'' an HMM for 
$N$ time points denoted by $t=1, ..., N$, the HMM could ``generate'' a sequence of 
observations $x_1 ... x_N$, where each $x_i$ is an output symbol by 
going through a sequence of {\em hidden states} $w_1 ... w_N$ where $w_i \in U$ is a state. 
The association of such a latent sequence of state transitions with the observed symbols makes
it possible to use HMM to ``decode'' the latent behavior of students behind the surface behavior we directly observe in the log data, allowing for understanding student behavior more deeply than a model with no latent state variables. 

In many ways, the generation process behind an HMM is meant to simulate the
actual behavior of a student. We may say that students transition through different ``task states'' (or ``behavior states'')  in the process of study. 
One such task state may be to learn about a topic by mostly watching
lecture videos, another task state may be to work on quizzes, and yet
another may be to participate in forum discussions. While in each of these
different states, the student would tend to exhibit different surface
``micro'' behaviors. For example, in the lecture study state, the student
would tend to have many video-watching related behaviors and occasionally
forum activities, while in the quiz-taking state (in order to pass each
module), the student would tend to show many quiz-related ``micro''
activities as well as asking questions or checking discussions on the
forum. Note that due to the complexity of the student behavior, it is very
difficult to accurately {\em prescribe} the specific surface ``micro''
behavior patterns for each state in advance, especially without  prior knowledge about the students. For example, forum activities are likely interleaved with other activities   in every task state and the interleaving pattern can be somewhat irregular with potentially many variations. 
The major motivations for using an HMM are that 
\begin{enumerate*}[label=(\arabic*)]
    \item it uses a probabilistic model (i.e., the output probability distribution $p(x|u)$ conditioned on each state) to directly capture the inevitable uncertainty in the association of surface ``micro'' activities with their corresponding latent task/behavior state, which is often our main target to discover and characterize, and 
    \item it does not make any assumption about which latent task/behavior
      state must be associated with which observed activities or how a
      student would move from one state to another, but instead allows our
      data to ``tell'' us what kind of associations are most likely,
      what kind of transitions are most probable, and which states tend to be more long-lasting for any particular set of students. 
\end{enumerate*}

However, if we use an ordinary HMM to analyze our data, we would treat each
observed ``micro'' activity (e.g., video watching, or forum post reading)
as an output symbol, and thus the output distribution $p(x|u)$ for each
discovered latent state would be a simple distribution over all kinds of
observable micro activities recorded in our log data (e.g., 50\% lecture
watching, 8\% quiz taking, 7\% quiz submission, 2\% course wiki reading,
...). While such a distribution is meaningful and can already help us
interpret the corresponding latent state, it only gives us a rather
superficial characterization of student behavior.

Ideally, we want $p(x|u)$ to characterize the directly observable ``micro''
behavior in more detail to further capture the relations and dependencies
of these micro activities. To this end, we would treat an {\em entire
sequence} of micro activities (e.g., one session of activities) as an
observed ``symbol'' from a latent state, and further model the generation
of such a sequence with another Markov model where we treat each micro
activity as an {\em observable} state, and model the transitions between
these activity states in very much the same way as the state transitions in
HMM.

Adding this second layer would allow us to characterize a latent task
state in much more detail, as it would reveal not only what activities are
most common to a task state, but also the transition patterns between these
``micro'' activities (e.g., it can reveal frequent back-and-forth
transitions between quiz-taking and quiz-submission, which would suggest a
concentrated period of taking quizzes). Combining this ``surface'' Markov
model over the ``micro'' actions with the ``deep'' hidden Markov model over
the latent task states gives us a fairly general and powerful two-layer HMM
(TL-HMM) that can simultaneously learn ``deeply'' the latent task/behavior
states and their transitions as well as the corresponding ``micro''
activity transition patterns associated with each latent state to
facilitate interpretation and analysis of the discovered latent state
patterns. Next, we present this model more formally and discuss how to
estimate its parameters to uncover these latent patterns in an unsupervised
manner.

\subsection{Formal definition of TL-HMM}

Given a MOOC log, we can define a set $\mathbf{A}$ of actions that a
student can take at any given time. For example, an action $a \in
\mathbf{A}$ might be ``viewing lecture'', ``taking quiz'', or ``viewing
forum''. For each student in the course $\ell \in \mathbf{L}$, we then
extract a list of action sequences $\mathbf{S}_\ell$ that he or she
produced as observed in the log, where each sequence $\mathbf{s} \in
\mathbf{S}_\ell$ is itself a list of actions $(a_1, a_2, \ldots, a_M)$ with
each $a_i \in \mathbf{A}$.  Each sequence can be divided flexibly; in this
paper we chose to denote the end of a sequence as occurring when no further
actions occur within a 10 hour window of time (and thus our sequences
roughly correspond to one day's worth of activity).

Each sequence $\mathbf{s} \in \mathbf{S}_\ell$ is associated with a latent
state $u \in \{1,\ldots,K\}$ (where $K$ is a fixed constant picked in
advance). The actions within the sequence $\mathbf{s} = (a_1, a_2, \ldots,
a_M)$ are then modeled as a first-order Markov process conditioned upon $u$
where each action is drawn from a distribution conditioned upon the
previous action (except for the first which is sampled from an initial
starting distribution). We can write the parameters for the first-order
Markov model associated with latent state $u$ as $\lambda^{(u)} =
(\pi^{(u)}, A^{(u)})$ where $\pi^{(u)}$ indicates the initial probability
vector of length $|\mathbf{A}|$ and $A^{(u)}$ is an $|\mathbf{A}| \times
|\mathbf{A}|$ matrix indicating the transition probabilities between each
pair of actions from $\mathbf{A}$.

Thus, the probability of a sequence $\mathbf{s}$ of length $M$ given its
latent state $u$ is
\begin{equation}
  P(\mathbf{s} \mid \lambda^{(u)}) = P(a_1 \mid \pi^{(u)}) \prod_{i=2}^M P(a_i \mid
  a_{i-1}, A^{(u)})
\end{equation}
where $P(a \mid \pi^{(u)}) = \pi^{(u)}_a$ is the probability of starting
with action $a$ and $P(a_i \mid a_{i-1}, A^{(u)}) = A^{(u)}_{a_{i-1}, a_i}$
is the transition probability of moving from action $a_{i-1}$ to $a_{i}$.

We can compute the likelihood of a list of action sequences
$\mathbf{S}_\ell$ of length $N$ for a student $\ell$ by marginalizing over
all possible latent state sequences $U=(v_1, \ldots, v_N)$ as
\begin{equation}
  \begin{split}
  P(\mathbf{S}_\ell \mid \Lambda)
  &= \sum_U \left(P(v_1 \mid \Lambda) P(\mathbf{s}_1 \mid \lambda^{(v_1)})
  \phantom{\prod_{i=2}^N}\right.\\
  &\quad \left.\times \prod_{i=2}^N P(v_i \mid v_{i-1}, \Lambda)
  P(\mathbf{s}_i \mid \lambda^{(v_i)})\right)
  \end{split}
\end{equation}
where $\Lambda$ is the set of all model parameters.  In our model, we let
$\Lambda = (\pi, A, \lambda^{(1)}, \ldots, \lambda^{(K)})$ where $\pi$ and
$A$ are the parameters of a first-order Markov model over the latent states
and each $\lambda^{(i)}$ consists of the parameters for the first-order
Markov model over action sequences for latent state $i$. Thus $\pi$
(without superscripts) is an initial probability vector of length $K$ and
$A$ (without superscripts) is a $K \times K$ transition probability matrix,
analogous to the case with the individual first-order Markov model
parameters $\lambda^{(i)}$ for each latent state.

This can be seen as a modification of the traditional hidden Markov
model~\cite{Rabiner:1990:RSR} where instead of discrete observations (one
for each latent state transition) we have observations that take the form
of entire sequences $\mathbf{s}_i = (a_1, a_2, \ldots a_M)$ whose
probabilities are computed using another (non-hidden) Markov model
conditioned upon the latent state $u_i$.

\subsection{Parameter Estimation}
To learn the parameters of our model, we may attempt to use maximum
likelihood estimation. Unfortunately, a closed-form solution does not
exist, so we must appeal to the EM algorithm~\cite{Dempster:1977:JRSS}. In
particular, we propose a minor modification of the Baum-Welch
algorithm~\cite{Rabiner:1990:RSR} which is used to learn the parameters for
hidden Markov models. In the following sections, we will provide a brief
description of the original Baum-Welch algorithm, and then describe our
modification.

\subsubsection{Baum-Welch for Traditional HMMs}
In the traditional HMM formulation, we have $\Lambda = (\pi, A, B)$ where
$\pi$ is the initial probability distribution over the latent states,
$A$ is a $N\times N$ matrix indicating the latent state transition
probabilities, and $B$ is a $N \times V$ matrix indicating the probability
of generating a discrete observation symbol from a given latent state. The
Baum-Welch algorithm (also called the forward-backward algorithm) defines
two sets of variables $\alpha_t(i)$ called the forward variables and
$\beta_t(i)$ called the backward variables.

$\alpha_t(i) = P(o_1, \ldots, o_t, q_t = i \mid \Lambda)$ is the probability of
generating the sequence of observations $(o_1, o_2, \ldots, o_t)$ up to time
$t$ and arriving in state $i$ at that time. They are typically defined
using the following recursion:
\begin{itemize}
  \item $\alpha_1(i) = \pi_i b_i(o_1)$, the probability of starting in
    state $i$ ($\pi_i$) times the probability of generating the first
    observation $o_1$ from state $i$.
  \item $\alpha_{t+1}(i) = b_i(o_{t+1})\sum_{j=1}^N \alpha_t(j) A_{ji}$,
    the probability of generating the observation $o_{t+1}$ from state $i$
    times the probability that we arrive in state $i$ from any other
    previous state after generating all of the other observations.
\end{itemize}

Analogously, $\beta_t(i) = P(o_{t+1}, \ldots, o_{T} \mid q_t = i, \Lambda)$
is the probability of generating the \emph{rest of the sequence} given that
we are in state $i$ at time $t$. They are also defined using a recursion:
\begin{itemize}
  \item $\beta_T(i) = 1$
  \item $\beta_t(i) = \sum_{j=1}^N \beta_{t+1}(j) A_{ij} b_j(o_{t+1})$, the
    probability of transitioning to any state $j$ and generating the
    observation $o_{t+1}$ times the probability of generating the rest of
    the sequence given that we transitioned to state $j$.
\end{itemize}

Given the $\alpha$s and the $\beta$s, we can compute $\gamma_t(i)$, the posterior
probability of being in a given state $i$ at time $t$, and $\xi_t(i,j)$,
the posterior probability of going through a transition from state $i$ to
state $j$ at time $t$ as
\begin{equation}
  \gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N
  \alpha_t(j)\beta_t(j)}
\end{equation}
and
\begin{equation}
  \xi_t(i,j) = \frac{\alpha_t(i) A_{ij} b_j(o_{t+1}) \beta_{t+1}(j)}
  {\sum_{j=1}^N \alpha_t(j)\beta_t(j)}
\end{equation}
respectively. This is the E-step of the EM algorithm.

Given $\gamma_t(i)$ and $\xi_t(i,j)$ for each sequence $\mathbf{o}_k$ in
some training data of length $M$, we can update our model parameters
$\Lambda$ as
\begin{align}
  \pi_i &= \frac{\sum_{k=1}^M \gamma^{(\mathbf{o}_k)}_1(i)}{M},\\
  A_{ij} &= \frac{\sum_{k=1}^M \sum_{t=1}^T \xi^{(\mathbf{o}_k)}_t(i,j)}
  {\sum_{k=1}^M \sum_{j=1}^N \sum_{t=1}^T \xi^{(\mathbf{o}_k)}_t(i,j)},
  \text{ and }\\
  b_i(v) &= \frac{\sum_{k=1}^M \sum_{t=1, o_{k,t} = v}^T
  \gamma^{(\mathbf{o}_k)}_t(i)}
  {\sum_{k=1}^M \sum_{t=1}^T \gamma^{(\mathbf{o}_{k,t})}_t(i)}.
\end{align}
This is the M-step of the EM algorithm.

\subsubsection{Baum-Welch for HMMs with Sequence Observations}
The major deviation of our model from the traditional HMM is that our
observations are themselves sequences. Thus, in all of the equations
presented in the previous section, we treat an observation $o_t =
\mathbf{s}_t$ at time $t$ as consisting itself of a list of actions $(a_1,
\ldots, a_M)$. Thus, instead of defining $b_i(v)$ as the probability that a
discrete observation \emph{token} $v$ is generated from state $i$, we have
\begin{equation}
  b_i(\mathbf{s}_t) = P(\mathbf{s}_t \mid \lambda^{(i)})
  = P(a_1 \mid \pi^{(i)}) \prod_{k=2}^M P(a_k \mid
  a_{k-1}, A^{(i)})
\end{equation}
where $b_i(\mathbf{s})$ now defines the probability of an action sequence
using a first-order (non-hidden) Markov model.

Fortunately, the recursions for the $\alpha$ and $\beta$ variables remain
the same, as do the definitions of $\gamma_t(i)$ and $\xi_t(i,j)$ in the
E-step. The only change is in the updating equations in the M-step,
where we replace the update for $b_i(v)$ by a pair of updates: one for
$\pi^{(i)}_a$ one for $A^{(i)}_{ab}$. We denote the $m$-th action in the
$t$-th sequence for the $k$-th student as $o_{k,t,m}$. The two updates can
be written as
\begin{align}
  \pi^{(i)}_{a}
  &= \frac{\sum_{k=1}^M \sum_{t=1,a_1 = a}^T \gamma^{(\mathbf{o}_k)}_t(i)}
  {\sum_{k=1}^M \sum_{t=1}^T \gamma^{(\mathbf{o}_k)}_t(i)}, \text{ and }\\
  A^{(i)}_{ab}
  &= \frac{\sum_{k=1}^M \sum_{t=1}^T
  \sum_{m=2,a_{m-1}=a \land a_m = b}^{|\mathbf{o}_{k,t}|}
  \gamma^{(\mathbf{o}_k)}_t(i)}
  {\sum_{k=1}^M \sum_{t=1}^T \sum_{m=2}^{|\mathbf{o}_{k,t}|}
  \gamma^{(\mathbf{o}_k)}_t(i)}
\end{align}
where $a_m = o_{t,k,m}$ (for notational conciseness). Our modified EM
algorithm for HMMs with sequence observations is provided as part of the
\meta/ toolkit~\citemeta{}.
