\section{Model}

Given a MOOC log, we can define a set $\mathbf{A}$ of actions that a
student can take at any given time. For each student $\ell \in \mathbf{L}$,
we then extract a list of action sequences $\mathbf{S}_\ell$ that he or she
produced as observed in the log. Each sequence can be divided flexibly; in
this paper we chose to denote the end of a sequence as occurring when no
further actions occur within a 10 hour window of time (and thus our
sequences roughly correspond to one day's worth of activity). Each action
sequence $s_i \in \mathbf{S}_\ell$ is a list of actions from $\mathbf{A}$.

Each sequence $s_i$ is associated with a latent state $u_i$, and the
actions within the sequence $(a_1, a_2, \ldots, a_{|s_i|})$ are modeled as
a first-order Markov process, where each action is drawn from a
distribution conditioned upon the previous action. (There is also an initial
distribution that governs the probability of starting the sequence with a
particular action $a_1 \in A$.)

For a given list of action sequences $\mathbf{S}_\ell$, we have the
likelihood function
\begin{equation}
  P(\mathbf{S}_\ell \mid \Lambda)
  = \prod_{s_i \in \mathbf{S}_\ell}
  \sum_{U} P(u_i \mid u_{i-1})
  \prod_{a_t \in s_i}
  P(a_t \mid a_{t-1}, u_i)
\end{equation}
where $\Lambda$ is the set of parameters for our model, and the summation
over $U$ refers to summing over all possible latent state sequences of the
same length as $\mathbf{S}_\ell$. (For $i=1$ and for $t=1$, we define the
transition probability as drawing from the initial distribution; this can
be thought of as padding the sequences with a dummy start symbol $u_0$ and
$a_0$, respectively). This can be seen as a modification of the traditional
hidden Markov model~\cite{Rabiner:1990:RSR} where instead of discrete
observations (one for each latent state transition) we have observations
that take the form of entire sequences $(a_1, a_2, \ldots a_T)$ whose
probabilities are computed using another (non-hidden) Markov model
conditioned upon the latent state $u_i$.

\subsection{Parameter Estimation}
To learn the parameters of our model, we may attempt to use maximum
likelihood estimation. Unfortunately, a closed-form solution does not
exist, so we must appeal to the EM algorithm~\cite{Dempster:1977:JRSS}. In
particular, we propose a minor modification of the Baum-Welch
algorithm~\cite{Rabiner:1990:RSR} which is used to learn the parameters for
hidden Markov models. In the following sections, we will provide a brief
description of the original Baum-Welch algorithm, and then describe our
modification.

\subsubsection{Baum-Welch for Traditional HMMs}
In the traditional HMM formulation, we have $\Lambda = (A, B, \pi)$ where
$A$ is a $N\times N$ matrix indicating the latent state transition
probabilities, $B$ is a $N \times V$ matrix indicating the probability of
generating an observation symbol from a given latent state, and $\pi$ is
the initial probability distribution over the latent states.

The Baum-Welch algorithm (also called the forward-backward algorithm)
defines two sets of variables $\alpha_t(i)$ called the forward variables
and $\beta_t(i)$ called the backward variables.

$\alpha_t(i) = P(o_1, \ldots, o_t, q_t = i \mid \Lambda)$ is the probability of
generating the sequence of observations $(o_1, o_2, \ldots, o_t)$ up to time
$t$ and arriving in state $i$ at that time. They are typically defined
using the following recursion:
\begin{itemize}
  \item $\alpha_1(i) = \pi_i b_i(o_1)$, the probability of starting in
    state $i$ ($\pi_i$) times the probability of generating the first
    observation $o_1$ from state $i$.
  \item $\alpha_{t+1}(i) = b_i(o_{t+1})\sum_{j=1}^N \alpha_t(j) a_{ji}$,
    the probability of generating the observation $o_{t+1}$ from state $i$
    times the probability that we arrive in state $i$ from any other
    previous state after generating all of the other observations.
\end{itemize}

Analogously, $\beta_t(i) = P(o_{t+1}, \ldots, o_{T} \mid q_t = i, \Lambda)$
is the probability of generating the \emph{rest of the sequence} given that
we are in state $i$ at time $t$. They are also defined using a recursion:
\begin{itemize}
  \item $\beta_T(i) = 1$
  \item $\beta_t(i) = \sum_{j=1}^N \beta_{t+1}(j) a_{ij} b_j(o_{t+1})$, the
    probability of transitioning to any state $j$ and generating the
    observation $o_{t+1}$ times the probability of generating the rest of
    the sequence given that we transitioned to state $j$.
\end{itemize}

Given the $\alpha$s and the $\beta$s, we can compute the posterior
probability of being in a given state $i$ at time $t$ and the posterior
probability of going through a transition from state $i$ to state $j$ at
time $t$ as
\begin{equation}
  \gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N
  \alpha_t(j)\beta_t(j)}
\end{equation}
and
\begin{equation}
  \xi_t(i,j) = \frac{\alpha_t(i)a_{ij} b_j(o_{t+1}) \beta_{t+1}(j)}
  {\sum_{j=1}^N \alpha_t(j)\beta_t(j)}
\end{equation}
respectively. These probabilities are enough to update our model parameters
$\Lambda$ as
\begin{align}
  \pi_i &= \gamma_1(i),\\
  a_{ij} &= \frac{\sum_{t=1}^T \xi_t(i,j)}{\sum_{j=1}^N \sum_{t=1}^T
  \xi_t(i,j)}, \text{ and }\\
  b_i(v) &= \frac{\sum_{t=1, o_t = v}^T \gamma_t(i)}
  {\sum_{t=1}^T \gamma_t(i)}.
\end{align}

% TODO: update this to compute \Lambda across multiple observation
% sequences

\subsubsection{Baum-Welch for HMMs with Sequence Observations}
The major deviation of our model from the traditional HMM is that our
observations are themselves sequences. Thus, in all of the equations
presented in the previous section, we treat an observation $o_t$ at time
$t$ as consisting itself of a list of actions $a_1, \ldots, a_M$. Thus,
instead of defining $b_i(v)$ as the probability that an observation
\emph{token} $v$ is generated from state $i$, we have
\begin{equation}
  b_i(a_1, \ldots, a_M) = P(a_1) \prod_{i=2}^M P(a_i \mid a_{i-1})
\end{equation}
where $b_i(\mathbf{o})$ now defines the probability of an action sequence
using a first-order (non-hidden) Markov model. Fortunately, the recursions
for the $\alpha$ and $\beta$ variables remain the same, as do the
definitions of $\gamma_t(i)$ and $\xi_t(i,j)$. The only change, then, is in
the updating equations, where we replace the update for $b_i$ by a pair of
updates

% TODO: add the pair of updates
