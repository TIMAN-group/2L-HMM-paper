\section{Limitations and Potential Drawbacks}

There are a few limitations of our model that are important to highlight.
First, there are specific technical limitations due to the statistical
nature of the model and the particular methodology we propose for fitting
our model parameters. Second, there are limitations in the kinds of
patterns our model is able to discover in its current formulation, and the
ease with which instructors are able to extract knowledge from these
patterns. We discuss both of these lines below.

\subsection{Technical Limitations and Implementation Challenges}

 %irregular behavior
%patterns In particular, because
One potential limitation is that the model is complex and has many parameters,
to truly uncover relevant patterns in the data, thus a large amount of it must
be available to the training algorithm. The assumption that we have a large
amount of sequence data available for training on should generally hold for
most MOOC courses, but this assumption may be problematic if attempting to
apply our model to smaller online (or on-campus) courses.

Our model fits its parameters using maximum likelihood estimation using the
EM algorithm. The EM algorithm is a hill climbing algorithm that is
optimizing in a highly non-convex parameter space. Thus, it can only
guarantee reaching a local maximum in practice~\citep{Dempster:1977:JRSS}.
This may mean that the parameters found for the model may not be the
``best'' parameters in a global sense, which may lead to suboptimal latent
state representations and transition patterns. The problem may not be as serious as it 
appears to be because strong patterns tend to always show up as long as the algorithm reaches a reasonably good local maximum, and the differences of the results tend to be 
related to weak ``unstable'' patterns which may not be reliable anyway. 
 A commonly applied approach
to address the problem of multiple local maxima is to run the model multiple times with different
starting points to allow the model to explore a larger portion of the
potential parameter space. One can then compare the log-likelihood of the
data between the models that were started from different initialization
points and select the one that has the highest value. This still does not
guarantee finding a global maximum, but it does help alleviate the
potential for finding a particularly bad local maximum.  
In practice, we
have found that our model can be fit to the data fairly quickly on
commodity hardware, so running it multiple times to address this concern is
not computationally unfeasible. 


A further complication in blindly applying the EM algorithm to our model is
the fact that the observation probabilities will be incredibly small. The
probability that a specific sequence is generated by a specific Markov
chain (one of our latent states) will decrease exponentially with its
length. While there are general approaches to avoiding numerical underflow
in hidden Markov models, applying the ``scaling'' method proposed by
\citet{Rabiner:1990:RSR} will still result in numeric stability issues in
our case due to the incredibly small sequence-generation probabilities. We
address this in our open-source implementation by computing the trellises
in log-space and using the log-sum-exp trick when we need to take
summations, which exploits the identity
\begin{equation}
    \log \sum_{i=1}^n e^{x_i} = a + \log \sum_{i=1}^n e^{x_i - a}
\end{equation}
where $a$ is typically set to $\max_i x_i$ to improve stability. We did not
encounter further stability issues once we applied these two tricks.

As is the case with traditional hidden Markov models, it is often important
to smooth the underlying model's distributions to ensure that there is a
non-zero probability of generating the observations. We employed a simple
additive smoothing in our implementation with a small additive constant for
all of our transition matrices to avoid this problem.

\subsection{Limitations of Discovered Patterns}

The proposed behavior representation is most suitable for representing
recurring behavior patterns, which presumably are most interesting 
to extract from the data, but may not cover all the interesting patterns in the data. 
It would be interesting to further explore other complementary models such as
 time series models, which may help capture non-recurring patterns.  
 
Our proposed model is flexible in the patterns it can discover in two main
ways. The first is that the granularity of the patterns can be adjusted by
changing the segmentation strategy one uses to divide the user action
stream into discrete ``sessions''. The other is in the number of latent
states $K$ that are used to describe the segmented action sequences. One
drawback of these two lines of flexibility is that there is not necessarily
a clear answer as to the ``correct'' approach for both in any given
scenario. Varying the segmentation strategy changes the granularity of the
patterns that are required to be explained by the latent states, which will
change their meaning. Varying the number of latent states increases the
flexibility of the model, but also may result in latent states that are not
substantially different from the other latent states and/or latent states
with very low probability. This flexibility forces a user of our model to
make some assumptions about the patterns they wish to find (granularity,
diversity), and the model itself does not necessarily provide clear
guidance as to what the best approach is.

Furthermore, we have made an implicit assumption that the segmentation
strategy is applied as a pre-processing step (and is most obviously a
deterministic process). The proposed segmentation strategies in this paper
do not specifically allow for the transitioning between the different
latent behavior patterns to occur over different windows of time for
different users: we have made a strong assumption that transitions between
latent states only occur at action sequence boundaries. One can model how
much time a user stays in a given state in terms of the number of sessions
they remain there before transitioning, but it may be better to directly
model the amount of time we expect a user to stay in the given state
directly. In other words, a more powerful model might be one in which the
segmentation and the latent behavior pattern discovery are jointly modeled
in a single probabilistic framework rather than being completely separate
pieces as we investigate in this work.

While the model can discover \emph{patterns} in the data in a fully
automated way, there is still clearly a burden on the user of our model to
interpret the patterns it has discovered to create actionable
\emph{knowledge} about the MOOC from which the data was extracted.
Extracting the patterns is a necessary step towards the creation of
knowledge, and we view our model as a component in a collaborative system
which leverages the machine to perform statistical modeling to extract
patterns which then enable a human actor to extract knowledge and take
actions on the basis of the data. The pattern discovery is an important
and absolutely necessary component, without which it would be very difficult, if not impossible, 
for humans to directly digest the student behavior buried in the large amount of 
data.  Of course, pattern discovery is only a means to help humans obtain knowledge, not the end of the knowledge discovery process.
