\section{Related Work}

Our model is based heavily on the prior art of Hidden Markov Models
(HMMs)~\cite{Rabiner:1990:RSR} for sequence labeling tasks. As a member of
the more general family of probabilistic graphical
models~\cite{Koller:2009:PGM}, HMMs are widely applicable and have been
used for tasks such as speech recognition~\cite{Huang:1990:HMM},
part-of-speech tagging~\cite{Jurafsky:2009:SLP}, and
econometrics~\cite{Hamilton:1990:JoE}. A major challenge in applying HMMs
and other graphical models successfully to solve a problem is to design an
appropriate architecture of the model, which always varies according to
specific applications.

For example, in part-of-speech tagging~\cite{Jurafsky:2009:SLP}, the output
distributions are categorical (distributions over words from a fixed
vocabulary) and the latent states represent the part-of-speech category for
a word. In speech recognition~\cite{Huang:1990:HMM}, the output
distributions might be mixtures of Gaussians to predict real-valued vectors
extracted from short windows of a speech signal. In the domain of
econometrics, \citet{Hamilton:1990:JoE} explores HMMs in the context of
``regime-switching.'' In this framing, the goal is to understand how
econometric data changes by modeling discrete changes in ``regime'' as
having an impact on the resulting real-valued vector data observed. The
``regimes'' are thus represented with a model that can produce real-valued
vector data, such as a multivariate Gaussian or an auto-regressive model.
The analogy with HMMs is that a ``regime'' is a latent state, and the
characterization of the regime itself is the output distribution for that
latent state. Our model can be seen as such a ``regime-switching'' model
where the output of the ``regimes'' that students are switching between are
discrete-valued \emph{sequences} (as opposed to real numbers, vectors of
real numbers, or categorical symbols) and the model used to represent a
specific ``regime'' is an (observable) Markov chain over the observed
student actions. We view the switching between ``regimes'' as the first
``layer'' of our model, and the transitioning behavior \emph{within} a
``regime'' between the actions a student takes as the second ``layer'' of
our model.

A multi-layered approach to HMM modeling of sequence data has been
performed before in other domains. \citet{Zhang:2004:CVPR}, for example,
explored a two-layer HMM framework for modeling actions in meetings, but
their definition of ``two-layer'' differs from ours. In their formulation,
the ``lower-layer'' level is used to label audio-video action sequences
into basic events, and the ``upper-layer'' is used to label the output of
the lower-layer to discover higher-level office behavior abstractions.
\citet{Oliver:2004:CVIU} propose a similar layered HMM approach
for modeling office activity at multiple different levels of time
granularity. In their approach, each layer $L$ is represented as a ``bank''
of $K$ different HMMs that model sequences of some length $T_L$. At the
bottom layer ($L = 1$), the bank of $K$ HMMs corresponding to that layer is
run on some initial observation data, considering windows of observations
of length $T_1$. Then an output is generated by using the inferential
results of these $K$ HMMs to make a prediction: which of the $K$ HMMs was
most likely to produce that sequence of observations? This output is then
fed to the next layer of HMMs, which considers sequences of length $T_2$
and outputs prediction results as to which of the $K$ HMMs at layer two
were most likely to produce the sequence of outputs produced by the
previous layer, and so on.

Our formulation differs from both \citet{Zhang:2004:CVPR} and
\citet{Oliver:2004:CVIU} in that we do not feed the labeled sequence of the
lower level into the input of the higher level. Instead, our lower level is
treated using a non-hidden Markov model, and the higher level is modeling
transitions between the $K$ different non-hidden Markov models we consider.
The problem to be solved is similar in that we wish to predict a ``label''
for a sequence of actions a student takes as well as understand the
transition behavior between those labels.  However, one of the consequences
of modeling the lower layer using a non-hidden Markov model instead of an
HMM directly is that the meaning of the $K$ different latent states can be
more clearly captured by using our proposed behavior representation. If we
were to use an HMM as our first layer, the behavior patterns (like from
Figure~\ref{fig:motivating-example}) would instead have nodes that
represent another set of latent states instead of being concrete actions
themselves. Each of these latent states would then be associated with some
other output distribution over the possible concrete actions to be
considered. In order to understand a single behavior pattern uncovered, one
would first have to understand the different output distributions for the
latent states in that pattern to understand the meaning of that latent
state. This further complicates the understanding of higher level patterns
because understanding the higher layer patterns requires understanding the
lower layer patterns. By taking a more restrictive view of the first layer,
we can produce a representation that can be more readily interpreted due to
the states in our first layer representation having an immediately clear
meaning (the concrete action they represent).

The 2L-HMM model we propose is more closely related to the Hierarchical
Hidden Markov Model (HHMM) detailed in \citet{Fine:1998:ML}. Here, the
``layers'' are modeled by having the hidden Markov model have two kinds of
transitions.  Horizontal transitions move between states within a layer,
where vertical transitions move between different layers. At the bottom
layer lie the ``production'' states, which output symbols according to some
probability distribution. Our specific model, in this case, can be modeled as
an HHMM where the horizontal transitions between nodes at the highest layer
(including self-loops) \emph{must} be immediately followed by a vertical
transition to the lower layer. The output probability distributions over
symbols in the lower layer ``production'' level are forced to emit only one
kind of symbol, and vertical transitions are only allowed into the original
higher-layer state that transitioned down into the lower-layer.

Mixtures of hidden Markov models are also conceptually similar to our
formulation. \citet{Song:2009:NDSS} explored using a mixture of hidden
Markov models in the context of anomaly detection in the security domain.
\citet{Ypma:2002:Springer} use mixtures of HMMs to categorize web pages and
cluster users by investigating web log data, which is quite similar to the
clickstream log data we obtain from MOOCs. The major difference between
our approach and a standard mixture of HMMs is that we also model the
\emph{transition behavior} between the Markov models that make up our
model's lowest layer, where a standard mixture of HMMs would ignore the
potential dependence of the previous sequence's latent state on the next
sequence's latent state.

HMMs or similar ideas have been previously applied to model education
data~\cite{Shih:2010:EDM,Kizilcec:2013:LAK,Davis:2016:EDM}, but the
previous models are not well tuned toward the student behavior task and
thus cannot adequately address all the aspects of the complexity of student
learning behavior.  A main technical contribution of this paper is to
propose a more general HMM that can better adapt to the variations of
student behavior via its variable resolution and nested HMM structure, and
thus enable discovery of more sophisticated behavior patterns and provide
a more detailed characterization of student behavior than the previous
models.

For example, \citet{Kizilcec:2013:LAK} assigned students to states
following a rule-based approach based upon when the student submitted the
assignment for a particular week in the course. They investigated how
students transitioned between these states as the course progressed, and
used the sequence of states a student exhibited as a representation for
performing $k$-means clustering of students into related groups. This
differs from our method substantially: we assign students to states using a
probabilistic framework to account for uncertainty in this state
assignment and jointly learn \emph{representations} for these states,
which are treated as being \emph{latent} as opposed to pre-defined using
some rule (or set of rules).  Furthermore, our model provides more
flexibility in how the time segments are defined, allowing for both finer
(for example, day-by-day) or coarser (for example, month-by-month)
granularity. \citet{Shih:2010:EDM} investigated the use of HMM-based
clustering techniques for automatic discovery of student learning
strategies when solving a problem. This is similar to our approach in that
the description of behavior profiles is a Markov model, but cannot further
characterize each latent state with another informative HMM. Thus, their
work can be regarded to modeling ``micro'' behavior, whereas our model can
model both ``micro'' and ``macro'' behavior.

\citet{Davis:2016:EDM} investigate frequent student behavior pattern chains
with a set of actions that are defined similarly to ours. However, their
method for finding the common behavioral patterns involves a manual
clustering step to identify behavioral ``motifs,'' which is then followed
by an automatic (rule-based) assignment of all sequences to these motifs.
Our method, by contrast, attempts to do this automatically: the latent
state representations obtained by our model attempt to capture similar
meanings to their behavior motifs in a completely automated fashion. They
also automatically generate and investigate Markov models for different
MOOCs, but do so by considering \emph{all} student action sequences as
belonging to a \emph{single Markov model}. In our approach, we allow each
student behavior sequence to belong to one of $K$ different Markov models
(and further model the transition probabilities between these latent state
Markov models between each sequence a student generates). Thus, their
Markov models presented are a special case of our model when $K = 1$.

\citet{Faucon:2016:EDM} proposed a semi-Markov model for simulating MOOC
students. They produce behavior profiles that characterize groups of
students in the form of semi-Markov models like our proposed model does,
but they assume that a student can belong to only one behavior profile
across the entire course rather than allowing this profile to change over
time. Because we do not have this restriction, our model is also able to
learn the transition probabilities between the different behavior profiles
we discover.

There are a few additional related studies worth mentioning.
Bayesian Knowledge Tracing~\cite{Corbett:1994:UMUAI} in its basic form uses
a hidden Markov model to model the probability that a learner knows a
certain skill at a given time. Modifications to this algorithm include
contextual estimation of the ``slip'' and ``guess'' probabilities of the
model~\cite{Baker:2008:ITS} and most recently a re-framing as a neural
network problem~\cite{Piech:2015:NIPS}.

