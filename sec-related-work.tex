\section{Related Work}

Our model is based heavily upon the prior art of Hidden Markov
Models~\cite{Rabiner:1990:RSR} for sequence labeling tasks. These types of
models are widely applicable and have been used for tasks such as speech
recognition~\cite{Huang:1990:HMM}, part-of-speech
tagging~\cite{Jurafsky:2009:SLP}, and econometrics~\cite{Hamilton:1990:JoE}
and are a member of the more general family of probabilistic graphical
models~\cite{Koller:2009:PGM}.  A major challenge in applying HMMs and
other graphical models successfully to solve a problem is to design an
appropriate architecture of the model, which always varies according to
specific applications.

A multi-layered approach to HMM modeling of sequence data has been
performed before in other domains. \citet{Zhang:2004:CVPR}, for example,
explored a two-layer HMM framework for modeling actions in meetings, but
their definition of ``two-layer'' differs from ours. In their formulation,
the ``lower-layer'' level is used to label audio-video action sequences
into basic events, and the ``upper-layer'' is used to label the output of
the lower-layer to discover higher-level office behavior abstractions. Our
formulation differs in that we do not feed the labeled sequence of the
lower level into the input of the higher level. Instead, our lower level is
actually treated using a non-hidden Markov model, and the higher level is
modeling transitions between the $K$ different non-hidden Markov models we
consider.

Our formulation is more closely related to the Hierarchical Hidden Markov
Model (HHMM) detailed in \citet{Fine:1998:ML}. Here, the ``layers'' are
modeled by having the hidden Markov model have two kinds of transitions.
Horizontal transitions move between states within a layer, where vertical
transitions move between different layers. At the bottom layer lie the
``production'' states, which output symbols according to some probability
distribution. Our specific model in this case can be modeled as a HHMM
where the horizontal transitions between nodes at the highest layer
(including self-loops) \emph{must} be immediately followed by a vertical
transition to the lower layer. The output probability distributions over
symbols in the lower layer ``production'' level are forced to emit only one
kind of symbol, and vertical transitions are only allowed into the original
higher-layer state that transitioned down into the lower-layer.

Mixtures of hidden Markov models are also conceptually similar to our
formulation. \citet{Song:2009:NDSS} explored using a mixture of hidden
Markov models in the context of anomaly detection in the security domain.
\citet{Ypma:2002:Springer} use mixtures of HMMs to categorize web pages and
cluster users by investigating web log data, which is quite similar to the
clickstream log data we obtain from MOOCs. The major difference between
our approach and a standard mixture of HMMs is that we also model the
\emph{transition behavior} between the Markov models that make up our
model's lowest layer, where a standard mixture of HMMs would ignore the
potential dependence of the previous sequence's latent state on the next
sequence's latent state.

HMMs or similar ideas have been previously applied to model education
data~\cite{Shih:2010:EDM,Kizilcec:2013:LAK,Davis:2016:EDM}, but the
previous models are not well tuned toward the student behavior task and
thus cannot adequately address all the aspects of complexity of student
learning behavior.  A main technical contribution of this paper is to
propose a more general HMM that can better adapt to the variations of
student behavior via its variable resolution and nested HMM structure, and
thus enable discovery of more sophisticated behavior patterns and provide
more detailed characterization of student behavior than the previous
models.

For example, \citet{Kizilcec:2013:LAK} assigned students to states
following a rule-based approach based upon when the student submitted the
assignment for a particular week in the course. They investigated how
students transitioned between these states as the course progressed, and
used the sequence of states a student exhibited as a representation for
performing $k$-means clustering of students into related groups. This
differs from our method substantially: we assign students to states using a
probabilistic framework to account for uncertainty in this state
assignment, and jointly learn \emph{representations} for these states,
which are treated as being \emph{latent} as opposed to pre-defined using
some rule (or set of rules).  Furthermore, our model provides more
flexibility in how the time segments are defined, allowing for both finer
(for example, day-by-day) or coarser (for example, month-by-month)
granularity. \citet{Shih:2010:EDM} investigated the use of HMM-based
clustering techniques for automatic discovery of student learning
strategies when solving a particular problem.  This is similar to our
approach in that the description of behavior profiles is a Markov model,
but cannot further characterize each latent state with another informative
HMM. Thus their work can be regarded to modeling ``micro'' behavior,
whereas our model can model both ``micro'' and ``macro'' behavior.

\citet{Davis:2016:EDM} investigate frequent student behavior pattern chains
with a set of actions that is defined similarly to ours. However, their
method for finding the common behavioral patterns involves a manual
clustering step to identify behavioral ``motifs,'' which is then followed
by an automatic (rule-based) assignment of all sequences to these motifs.
Our method, by contrast, attempts to do this automatically: the latent
state representations obtained by our model attempt to capture similar
meanings to their behavior motifs in a completely automated fashion. They
also automatically generate and investigate Markov models for different
MOOCs, but do so by considering \emph{all} student action sequences as
belonging to a \emph{single Markov model}. In our approach, we allow each
student behavior sequence to belong to one of $K$ different Markov models
(and further model the transition probabilities between these latent state
Markov models between each sequence a student generates). Thus, their
Markov models presented are a special case of our model when $K = 1$.

\citet{Faucon:2016:EDM} proposed a semi-Markov model for simulating MOOC
students. They produce behavior profiles that characterize groups of
students in the form of semi-Markov models like our proposed model does,
but they assume that a student can belong to only one behavior profile
across the entire course rather than allowing this profile to change over
time. Because we do not have this restriction, our model is also able to
learn the transition probabilities between the different behavior profiles
we discover.

There are a few additional related studies worth mentioning.
Bayesian Knowledge Tracing~\cite{Corbett:1994:UMUAI} in its basic form uses
a hidden Markov model to model the probability that a learner knows a
certain skill at a given time. Modifications to this algorithm include
contextual estimation of the ``slip'' and ``guess'' probabilities of the
model~\cite{Baker:2008:ITS} and most recently a re-framing as a neural
network problem~\cite{Piech:2015:NIPS}.

