\section{A Two-Layer HMM for MOOC Log Analysis}

Our general idea is to use a probabilistic generative model to model 
the student activities as recorded in a MOOC log, which means we will 
assume that all the observed student activities are samples drawn (i.e., ``generated'') from 
a parameterized probabilistic model. We can then estimate the 
parameter values of the probabilistic model by fitting the model to a specific
MOOC log data set. The estimated parameter values could then be treated as 
the latent ``knowledge'' discovered from the data. Because such a generative model 
attempts to fit {\em all} the data, it enables us to discover interesting patterns
that can explain the {\em overall} behavior of a student or the {\em common} behavior patterns shared by many students. 

An HMM  is a specific probabilistic generative model with a ``built-in'' state transition system
that would control the data to be generated by the model, thus it is especially 
suitable for modeling sequence data~\cite{Rabiner:1990:RSR}. At any moment, the HMM would be in 
one of $k$ states $U=\{u_1, ...,u_k\}$, and at the next moment, the HMM would move to 
another state stochastically according to a transition matrix that specifies the probability of
going to state $u_i$ when the HMM is currently in state $u_j$, i.e., $p(u_i|u_j)$. 
When the HMM is in state $u$, the HMM can generate an observable data point $x$ 
according to an output probabilistic model $p(x|u)$. Thus if we ``run'' an HMM for 
$N$ time points denoted by $t=1, ..., N$, the HMM could ``generate'' a sequence of 
observations $x_1 ... x_N$, where each $x_i$ is an output symbol by 
going through a sequence of {\em hidden states} $w_1 ... w_N$ where $w_i \in U$ is a state. 
The association of such a latent sequence of state transitions with the observed symbols makes
it possible to use HMM to ``decode'' the latent behavior of students behind the surface behavior we directly observe in the log data, allowing for understanding student behavior more deeply than a model with no latent state variables. 

In many ways, the generation process behind an HMM is meant to simulate the
actual behavior of a student. We assume that students transition through different ``task states'' (or ``behavior states'')  in the process of study. 
One such task state may be to learn about a topic by mostly watching
lecture videos, another task state may be to work on quizzes, and yet
another may be to participate in forum discussions. While in each of these
different states, the student would tend to exhibit different surface
``micro'' behaviors. For example, in the lecture study state, the student
would tend to have many video-watching related behaviors and occasionally
forum activities, while in the quiz-taking state (in order to pass each
module), the student would tend to show many quiz-related ``micro''
activities as well as asking questions or checking discussions on the
forum. Note that due to the complexity of the student behavior, it is very
difficult to accurately {\em prescribe} the specific surface ``micro''
behavior patterns for each state in advance, especially without  prior knowledge about the students. For example, forum activities are likely interleaved with other activities   in every task state and the interleaving pattern can be somewhat irregular with potentially many variations. 
The major motivations for using an HMM are that 
\begin{enumerate*}[label=(\arabic*)]
    \item it uses a probabilistic model (i.e., the output probability distribution $p(x|u)$ conditioned on each state) to directly capture the inevitable uncertainty in the association of surface ``micro'' activities with their corresponding latent task/behavior state, which is often our main target to discover and characterize, and 
    \item it does not make any assumption about which latent task/behavior
      state must be associated with which observed activities or how a
      student would move from one state to another, but instead allows our
      data to ``tell'' us what kind of associations are most likely,
      what kind of transitions are most probable, and which states tend to be more long-lasting for any particular set of students. 
\end{enumerate*}

However, if we use an ordinary HMM to analyze our data, we would treat each
observed ``micro'' activity (e.g., video watching, or forum post reading)
as an output symbol, and thus the output distribution $p(x|u)$ for each
discovered latent state would be a simple distribution over all kinds of
observable micro activities recorded in our log data (e.g., 50\% lecture
watching, 8\% quiz taking, 7\% quiz submission, 2\% course wiki reading,
...). While such a distribution is meaningful and can already help us
interpret the corresponding latent state, it only gives us a rather
superficial characterization of student behavior.

Ideally, we want $p(x|u)$ to characterize the directly observable ``micro''
behavior in more detail to further capture the relations and dependencies
of these micro activities. To this end, we would treat an {\em entire
sequence} of micro activities (e.g., one session of activities) as an
observed ``symbol'' from a latent state, and further model the generation
of such a sequence with another Markov model where we treat each micro
activity as an {\em observable} state, and model the transitions between
these activity states in very much the same way as the state transitions in
HMM.

Adding this second layer would allow us to characterize a latent task
state in much more detail, as it would reveal not only what activities are
most common to a task state, but also the transition patterns between these
``micro'' activities (e.g., it can reveal frequent back-and-forth
transitions between quiz-taking and quiz-submission, which would suggest a
concentrated period of taking quizzes). Combining this ``surface'' Markov
model over the ``micro'' actions with the ``deep'' hidden Markov model over
the latent task states gives us a fairly general and powerful two-layer HMM
(TL-HMM) that can simultaneously learn ``deeply'' the latent task/behavior
states and their transitions as well as the corresponding ``micro''
activity transition patterns associated with each latent state to
facilitate interpretation and analysis of the discovered latent state
patterns. Our implementation of the learning algorithm for TL-HMMs is
included as part of the \meta/ toolkit~\citemeta{}.
